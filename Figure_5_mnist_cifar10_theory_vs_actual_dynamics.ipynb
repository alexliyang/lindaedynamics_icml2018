{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5: MNIST and CIFAR-10 theoretical DAE/WDAE learning dynamics compared to actual learning dynamics.\n",
    "\n",
    "This notebook provides the code to produce Figure 5 in the paper: \"Learning dynamics of linear denoising autoencoders\". (ICML 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import OrderedDict\n",
    "\n",
    "# custom imports\n",
    "from src.linear_ae_net.linear_ae_net import LinearAutoEncoder\n",
    "from src.linear_ae_net.dynamics import theoretical_learning_dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- MNIST ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# cast to tensor\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "train_set = MNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "x_train = train_set.train_data.numpy()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_mnist = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "print(x_train_mnist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 training loss:  44.02943\n",
      "iteration:  100 training loss:  24.8457825\n",
      "iteration:  200 training loss:  19.9702025\n",
      "iteration:  300 training loss:  16.7520825\n",
      "iteration:  400 training loss:  15.1346225\n",
      "iteration:  500 training loss:  13.7594\n",
      "iteration:  600 training loss:  12.7403\n",
      "iteration:  700 training loss:  11.8747925\n",
      "iteration:  800 training loss:  11.01980625\n",
      "iteration:  900 training loss:  10.36042\n",
      "iteration:  1000 training loss:  9.82152375\n",
      "iteration:  1100 training loss:  9.335228125\n",
      "iteration:  1200 training loss:  8.90901875\n",
      "iteration:  1300 training loss:  8.512016875\n",
      "iteration:  1400 training loss:  8.130021875\n",
      "iteration:  1500 training loss:  7.7814675\n",
      "iteration:  1600 training loss:  7.481425\n",
      "iteration:  1700 training loss:  7.23100875\n",
      "iteration:  1800 training loss:  7.01295875\n",
      "iteration:  1900 training loss:  6.80954125\n",
      "iteration:  2000 training loss:  6.6159675\n",
      "iteration:  2100 training loss:  6.434506875\n",
      "iteration:  2200 training loss:  6.266335\n",
      "iteration:  2300 training loss:  6.1103075\n",
      "iteration:  2400 training loss:  5.963655625\n",
      "iteration:  2500 training loss:  5.82291875\n",
      "iteration:  2600 training loss:  5.686739375\n",
      "iteration:  2700 training loss:  5.5573775\n",
      "iteration:  2800 training loss:  5.438425\n",
      "iteration:  2900 training loss:  5.331311875\n",
      "iteration:  3000 training loss:  5.234254375\n",
      "iteration:  3100 training loss:  5.1438315625\n",
      "iteration:  3200 training loss:  5.0570715625\n",
      "iteration:  3300 training loss:  4.9725096875\n",
      "iteration:  3400 training loss:  4.8900321875\n",
      "iteration:  3500 training loss:  4.810164375\n",
      "iteration:  3600 training loss:  4.7333828125\n",
      "iteration:  3700 training loss:  4.6597371875\n",
      "iteration:  3800 training loss:  4.5888696875\n",
      "iteration:  3900 training loss:  4.52027\n",
      "iteration:  4000 training loss:  4.4535559375\n",
      "iteration:  4100 training loss:  4.3885925\n",
      "iteration:  4200 training loss:  4.3254453125\n",
      "iteration:  4300 training loss:  4.2642359375\n",
      "iteration:  4400 training loss:  4.2050271875\n",
      "iteration:  4500 training loss:  4.1477778125\n",
      "iteration:  4600 training loss:  4.092355625\n",
      "iteration:  4700 training loss:  4.038588125\n",
      "iteration:  4800 training loss:  3.9863325\n",
      "iteration:  4900 training loss:  3.9355278125\n",
      "iteration:  0 training loss:  44.03042697218126\n",
      "iteration:  100 training loss:  25.368284651818275\n",
      "iteration:  200 training loss:  22.67399148365021\n",
      "iteration:  300 training loss:  20.274011541404725\n",
      "iteration:  400 training loss:  19.383715443458556\n",
      "iteration:  500 training loss:  18.915993811454772\n",
      "iteration:  600 training loss:  18.593345676002503\n",
      "iteration:  700 training loss:  18.249114031066895\n",
      "iteration:  800 training loss:  18.12155240375519\n",
      "iteration:  900 training loss:  17.986578490180968\n",
      "iteration:  1000 training loss:  17.864495932922363\n",
      "iteration:  1100 training loss:  17.75402252799988\n",
      "iteration:  1200 training loss:  17.692022073669435\n",
      "iteration:  1300 training loss:  17.662828166122438\n",
      "iteration:  1400 training loss:  17.632091106033325\n",
      "iteration:  1500 training loss:  17.588773841934206\n",
      "iteration:  1600 training loss:  17.539700268325806\n",
      "iteration:  1700 training loss:  17.497724455337526\n",
      "iteration:  1800 training loss:  17.466813261566163\n",
      "iteration:  1900 training loss:  17.443659216384887\n",
      "iteration:  2000 training loss:  17.425690887908935\n",
      "iteration:  2100 training loss:  17.411451425628663\n",
      "iteration:  2200 training loss:  17.400165107574463\n",
      "iteration:  2300 training loss:  17.39168949668884\n",
      "iteration:  2400 training loss:  17.385496344299316\n",
      "iteration:  2500 training loss:  17.38045906364441\n",
      "iteration:  2600 training loss:  17.375531046600344\n",
      "iteration:  2700 training loss:  17.370192074432374\n",
      "iteration:  2800 training loss:  17.36447427528381\n",
      "iteration:  2900 training loss:  17.358728815002443\n",
      "iteration:  3000 training loss:  17.35333594390869\n",
      "iteration:  3100 training loss:  17.348523724136353\n",
      "iteration:  3200 training loss:  17.344322438278198\n",
      "iteration:  3300 training loss:  17.340712282714843\n",
      "iteration:  3400 training loss:  17.337684529342653\n",
      "iteration:  3500 training loss:  17.335237517166135\n",
      "iteration:  3600 training loss:  17.33332583480835\n",
      "iteration:  3700 training loss:  17.33184600822449\n",
      "iteration:  3800 training loss:  17.33065424232483\n",
      "iteration:  3900 training loss:  17.32961245651245\n",
      "iteration:  4000 training loss:  17.328608356552124\n",
      "iteration:  4100 training loss:  17.327565325927736\n",
      "iteration:  4200 training loss:  17.32644358757019\n",
      "iteration:  4300 training loss:  17.32522259162903\n",
      "iteration:  4400 training loss:  17.323920698776245\n",
      "iteration:  4500 training loss:  17.322565939788817\n",
      "iteration:  4600 training loss:  17.321188174896243\n",
      "iteration:  4700 training loss:  17.31983149154663\n",
      "iteration:  4800 training loss:  17.318532634124757\n",
      "iteration:  4900 training loss:  17.317312591018677\n",
      "iteration:  0 training loss:  44.029425003964576\n",
      "iteration:  100 training loss:  25.093028117103575\n",
      "iteration:  200 training loss:  20.60909329881668\n",
      "iteration:  300 training loss:  17.952661716461183\n",
      "iteration:  400 training loss:  16.692834072113037\n",
      "iteration:  500 training loss:  15.717870426483154\n",
      "iteration:  600 training loss:  14.946163877067566\n",
      "iteration:  700 training loss:  14.364961808547974\n",
      "iteration:  800 training loss:  13.818726515655518\n",
      "iteration:  900 training loss:  13.430752911987305\n",
      "iteration:  1000 training loss:  13.112734630355835\n",
      "iteration:  1100 training loss:  12.85438805595398\n",
      "iteration:  1200 training loss:  12.632150832061768\n",
      "iteration:  1300 training loss:  12.42759500175476\n",
      "iteration:  1400 training loss:  12.246667866973876\n",
      "iteration:  1500 training loss:  12.095620542068481\n",
      "iteration:  1600 training loss:  11.972812099990845\n",
      "iteration:  1700 training loss:  11.870187334823608\n",
      "iteration:  1800 training loss:  11.7788968334198\n",
      "iteration:  1900 training loss:  11.696226827850342\n",
      "iteration:  2000 training loss:  11.622252879333496\n",
      "iteration:  2100 training loss:  11.556280937652588\n",
      "iteration:  2200 training loss:  11.497248880615235\n",
      "iteration:  2300 training loss:  11.443595524902344\n",
      "iteration:  2400 training loss:  11.393838982009887\n",
      "iteration:  2500 training loss:  11.347683861160277\n",
      "iteration:  2600 training loss:  11.305701904525757\n",
      "iteration:  2700 training loss:  11.268171831359863\n",
      "iteration:  2800 training loss:  11.234618629989624\n",
      "iteration:  2900 training loss:  11.20417592414856\n",
      "iteration:  3000 training loss:  11.176085830459595\n",
      "iteration:  3100 training loss:  11.149887320632935\n",
      "iteration:  3200 training loss:  11.125337797698975\n",
      "iteration:  3300 training loss:  11.102274375534058\n",
      "iteration:  3400 training loss:  11.080548059768677\n",
      "iteration:  3500 training loss:  11.060020702667236\n",
      "iteration:  3600 training loss:  11.040582518310547\n",
      "iteration:  3700 training loss:  11.022155498733522\n",
      "iteration:  3800 training loss:  11.004722731018067\n",
      "iteration:  3900 training loss:  10.988289800643921\n",
      "iteration:  4000 training loss:  10.97285649116516\n",
      "iteration:  4100 training loss:  10.958392358093262\n",
      "iteration:  4200 training loss:  10.944838952941893\n",
      "iteration:  4300 training loss:  10.932110449829102\n",
      "iteration:  4400 training loss:  10.920127170639038\n",
      "iteration:  4500 training loss:  10.908813633346558\n",
      "iteration:  4600 training loss:  10.898114968948363\n",
      "iteration:  4700 training loss:  10.887994298858644\n",
      "iteration:  4800 training loss:  10.878424737701415\n",
      "iteration:  4900 training loss:  10.869379483566284\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "num_samples = 50000\n",
    "epochs = 5000\n",
    "lr = 0.01\n",
    "reg_param = 0.5\n",
    "var_param = 0.5\n",
    "reg = [0.0, reg_param, 0.0]\n",
    "var = [0.0, 0.0, var_param]\n",
    "num_trials = 3\n",
    "hidden_dim = 256\n",
    "\n",
    "mnist_models = []\n",
    "\n",
    "# convert to pytorch tensors\n",
    "x_train_mnist = torch.from_numpy(x_train_mnist)\n",
    "\n",
    "# set seed\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(321)\n",
    "\n",
    "# train autoencoder network\n",
    "for t in range(num_trials):\n",
    "    laeModel = LinearAutoEncoder()\n",
    "    laeModel.train(x_train_mnist[:num_samples,], None, input_dim=784, n_epoch=epochs, \n",
    "                   hidden_dim=hidden_dim, learning_rate=lr, reg_param=reg[t], \n",
    "                   noise='Gaussian', noise_scale=var[t], verbose=True)\n",
    "    mnist_models.append(laeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute theoretical dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  num = (lam - g)*E\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:14: RuntimeWarning: overflow encountered in multiply\n",
      "  denom = xi*(E - 1) + (lam - g)/u0\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  uf = num/denom\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  E = np.exp((2*(lam - g)*t)/tau)\n"
     ]
    }
   ],
   "source": [
    "# compute MNIST dynamics\n",
    "x_train_mnist_np = x_train_mnist.cpu().numpy()\n",
    "theoretical_dynamics_mnist = theoretical_learning_dynamics(x_train_mnist_np[:num_samples, :], \n",
    "                                                             x_train_mnist_np[:num_samples, :], \n",
    "                                                             n_epoch=epochs, lr=lr, \n",
    "                                                             var=0, reg=0)\n",
    "theoretical_dynamics_reg_mnist = theoretical_learning_dynamics(x_train_mnist_np[:num_samples, :], \n",
    "                                                                 x_train_mnist_np[:num_samples, :], \n",
    "                                                                 n_epoch=epochs, lr=lr, \n",
    "                                                                 var=0, reg=reg_param)\n",
    "theoretical_dynamics_noise_mnist = theoretical_learning_dynamics(x_train_mnist_np[:num_samples, :], \n",
    "                                                                   x_train_mnist_np[:num_samples, :], \n",
    "                                                                   n_epoch=epochs, lr=lr, \n",
    "                                                                   var=var_param, reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## --- CIFAR-10 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "x_train shape: (50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = CIFAR10(root=\"../data\", train=True, transform=trans, download=True)\n",
    "x_train = train_set.train_data\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "x_train_cifar10 = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "print('x_train shape:', x_train_cifar10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 training loss:  441.4945\n",
      "iteration:  100 training loss:  70.669275\n",
      "iteration:  200 training loss:  69.72841666666666\n",
      "iteration:  300 training loss:  58.50202916666667\n",
      "iteration:  400 training loss:  55.80244166666667\n",
      "iteration:  500 training loss:  51.31717916666667\n",
      "iteration:  600 training loss:  50.850366666666666\n",
      "iteration:  700 training loss:  49.964591666666664\n",
      "iteration:  800 training loss:  47.663066666666666\n",
      "iteration:  900 training loss:  44.86217083333333\n",
      "iteration:  1000 training loss:  43.0299125\n",
      "iteration:  1100 training loss:  41.507079166666664\n",
      "iteration:  1200 training loss:  39.72484583333333\n",
      "iteration:  1300 training loss:  38.2270125\n",
      "iteration:  1400 training loss:  37.11287083333333\n",
      "iteration:  1500 training loss:  36.04869583333333\n",
      "iteration:  1600 training loss:  35.109366666666666\n",
      "iteration:  1700 training loss:  34.52857083333333\n",
      "iteration:  1800 training loss:  34.24288958333333\n",
      "iteration:  1900 training loss:  34.055233333333334\n",
      "iteration:  2000 training loss:  33.8441375\n",
      "iteration:  2100 training loss:  33.561995833333334\n",
      "iteration:  2200 training loss:  33.20281666666666\n",
      "iteration:  2300 training loss:  32.791122916666666\n",
      "iteration:  2400 training loss:  32.36576041666667\n",
      "iteration:  2500 training loss:  31.956020833333334\n",
      "iteration:  2600 training loss:  31.571322916666666\n",
      "iteration:  2700 training loss:  31.20479375\n",
      "iteration:  2800 training loss:  30.838466666666665\n",
      "iteration:  2900 training loss:  30.452283333333334\n",
      "iteration:  3000 training loss:  30.036891666666666\n",
      "iteration:  3100 training loss:  29.60026875\n",
      "iteration:  3200 training loss:  29.16324375\n",
      "iteration:  3300 training loss:  28.748379166666666\n",
      "iteration:  3400 training loss:  28.37034583333333\n",
      "iteration:  3500 training loss:  28.032945833333333\n",
      "iteration:  3600 training loss:  27.73181875\n",
      "iteration:  3700 training loss:  27.458910416666665\n",
      "iteration:  3800 training loss:  27.20558541666667\n",
      "iteration:  3900 training loss:  26.964258333333333\n",
      "iteration:  4000 training loss:  26.728914583333335\n",
      "iteration:  4100 training loss:  26.495245833333332\n",
      "iteration:  4200 training loss:  26.260479166666666\n",
      "iteration:  4300 training loss:  26.023254166666668\n",
      "iteration:  4400 training loss:  25.7836\n",
      "iteration:  4500 training loss:  25.5429\n",
      "iteration:  4600 training loss:  25.303583333333332\n",
      "iteration:  4700 training loss:  25.068525\n",
      "iteration:  4800 training loss:  24.840364583333333\n",
      "iteration:  4900 training loss:  24.620997916666667\n",
      "iteration:  0 training loss:  441.5025608086333\n",
      "iteration:  100 training loss:  71.17775257964134\n",
      "iteration:  200 training loss:  70.46724305351576\n",
      "iteration:  300 training loss:  59.7789707095623\n",
      "iteration:  400 training loss:  57.63111220423381\n",
      "iteration:  500 training loss:  53.097897173754376\n",
      "iteration:  600 training loss:  52.5027664844195\n",
      "iteration:  700 training loss:  52.1005242752711\n",
      "iteration:  800 training loss:  50.87909505701065\n",
      "iteration:  900 training loss:  48.64271213353475\n",
      "iteration:  1000 training loss:  46.65340655689239\n",
      "iteration:  1100 training loss:  45.44541282704671\n",
      "iteration:  1200 training loss:  44.33972841726939\n",
      "iteration:  1300 training loss:  43.07054429740906\n",
      "iteration:  1400 training loss:  41.99888432776133\n",
      "iteration:  1500 training loss:  41.24018014640808\n",
      "iteration:  1600 training loss:  40.57124087556203\n",
      "iteration:  1700 training loss:  39.882245962905884\n",
      "iteration:  1800 training loss:  39.29178346697489\n",
      "iteration:  1900 training loss:  38.91947291158041\n",
      "iteration:  2000 training loss:  38.73690641237895\n",
      "iteration:  2100 training loss:  38.647075722503665\n",
      "iteration:  2200 training loss:  38.57994967015584\n",
      "iteration:  2300 training loss:  38.502252384567264\n",
      "iteration:  2400 training loss:  38.39904424438477\n",
      "iteration:  2500 training loss:  38.263669975026446\n",
      "iteration:  2600 training loss:  38.09630897496541\n",
      "iteration:  2700 training loss:  37.90502802302043\n",
      "iteration:  2800 training loss:  37.70387012964884\n",
      "iteration:  2900 training loss:  37.50712471516927\n",
      "iteration:  3000 training loss:  37.32398899739583\n",
      "iteration:  3100 training loss:  37.15797495155334\n",
      "iteration:  3200 training loss:  37.00922450434367\n",
      "iteration:  3300 training loss:  36.87546664085389\n",
      "iteration:  3400 training loss:  36.75167426172892\n",
      "iteration:  3500 training loss:  36.630599275970454\n",
      "iteration:  3600 training loss:  36.50499277089437\n",
      "iteration:  3700 training loss:  36.36981859296163\n",
      "iteration:  3800 training loss:  36.22357298622131\n",
      "iteration:  3900 training loss:  36.06839506823222\n",
      "iteration:  4000 training loss:  35.90922375704447\n",
      "iteration:  4100 training loss:  35.75222428131104\n",
      "iteration:  4200 training loss:  35.603161396662394\n",
      "iteration:  4300 training loss:  35.4660839205424\n",
      "iteration:  4400 training loss:  35.342844956080114\n",
      "iteration:  4500 training loss:  35.2333371538798\n",
      "iteration:  4600 training loss:  35.136185496266684\n",
      "iteration:  4700 training loss:  35.04945016962687\n",
      "iteration:  4800 training loss:  34.97115396614075\n",
      "iteration:  4900 training loss:  34.89965351511637\n",
      "iteration:  0 training loss:  441.4946667875771\n",
      "iteration:  100 training loss:  70.91873708278338\n",
      "iteration:  200 training loss:  69.9403482952277\n",
      "iteration:  300 training loss:  58.94432484102249\n",
      "iteration:  400 training loss:  56.15984598194758\n",
      "iteration:  500 training loss:  52.00851220699946\n",
      "iteration:  600 training loss:  51.60885277236303\n",
      "iteration:  700 training loss:  50.78670870304108\n",
      "iteration:  800 training loss:  48.61025219097137\n",
      "iteration:  900 training loss:  45.92310228417715\n",
      "iteration:  1000 training loss:  44.21079016278585\n",
      "iteration:  1100 training loss:  42.75586616522471\n",
      "iteration:  1200 training loss:  41.05672761910756\n",
      "iteration:  1300 training loss:  39.67301359532674\n",
      "iteration:  1400 training loss:  38.638588605181376\n",
      "iteration:  1500 training loss:  37.68421955121358\n",
      "iteration:  1600 training loss:  36.93636101277669\n",
      "iteration:  1700 training loss:  36.521880044746396\n",
      "iteration:  1800 training loss:  36.313870594215395\n",
      "iteration:  1900 training loss:  36.15066802647909\n",
      "iteration:  2000 training loss:  35.949967505455014\n",
      "iteration:  2100 training loss:  35.68395731201172\n",
      "iteration:  2200 training loss:  35.357396930948894\n",
      "iteration:  2300 training loss:  34.998168222554526\n",
      "iteration:  2400 training loss:  34.63923669509888\n",
      "iteration:  2500 training loss:  34.300100265375775\n",
      "iteration:  2600 training loss:  33.98292165031433\n",
      "iteration:  2700 training loss:  33.67704050585429\n",
      "iteration:  2800 training loss:  33.36537113965352\n",
      "iteration:  2900 training loss:  33.03469537188212\n",
      "iteration:  3000 training loss:  32.684695415878295\n",
      "iteration:  3100 training loss:  32.32856007919311\n",
      "iteration:  3200 training loss:  31.98524098294576\n",
      "iteration:  3300 training loss:  31.669931545384724\n",
      "iteration:  3400 training loss:  31.388966769282025\n",
      "iteration:  3500 training loss:  31.14061137148539\n",
      "iteration:  3600 training loss:  30.91867169303894\n",
      "iteration:  3700 training loss:  30.71570343437195\n",
      "iteration:  3800 training loss:  30.52486288172404\n",
      "iteration:  3900 training loss:  30.34066027463277\n",
      "iteration:  4000 training loss:  30.15915982424418\n",
      "iteration:  4100 training loss:  29.977861276626587\n",
      "iteration:  4200 training loss:  29.79556862373352\n",
      "iteration:  4300 training loss:  29.612253909428915\n",
      "iteration:  4400 training loss:  29.429061856206257\n",
      "iteration:  4500 training loss:  29.24802730992635\n",
      "iteration:  4600 training loss:  29.071624326324464\n",
      "iteration:  4700 training loss:  28.902151020304363\n",
      "iteration:  4800 training loss:  28.741247725550334\n",
      "iteration:  4900 training loss:  28.589618065897625\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "num_samples = 30000\n",
    "epochs = 5000\n",
    "lr = 0.001\n",
    "reg_param = 0.5\n",
    "var_param = 0.5\n",
    "reg = [0.0, reg_param, 0.0]\n",
    "var = [0.0, 0.0, var_param]\n",
    "num_trials = 3\n",
    "hidden_dim = 512\n",
    "\n",
    "cifar10_models = []\n",
    "\n",
    "# convert to pytorch tensors\n",
    "x_train_cifar10 = torch.from_numpy(x_train_cifar10)\n",
    "\n",
    "# set seed\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(321)\n",
    "\n",
    "# train autoencoder networks\n",
    "for t in range(num_trials):\n",
    "    laeModel = LinearAutoEncoder()\n",
    "    laeModel.train(x_train_cifar10[:num_samples,], None, input_dim=32*32*3, \n",
    "                   n_epoch=epochs, hidden_dim=hidden_dim, \n",
    "                   learning_rate=lr, reg_param=reg[t], \n",
    "                   noise='Gaussian', noise_scale=var[t], verbose=True)\n",
    "    cifar10_models.append(laeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute theoretical dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  num = (lam - g)*E\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:14: RuntimeWarning: overflow encountered in multiply\n",
      "  denom = xi*(E - 1) + (lam - g)/u0\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  uf = num/denom\n",
      "/home/canyon/Desktop/PhD/lindaedynamics_icml2018/src/linear_ae_net/dynamics.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  E = np.exp((2*(lam - g)*t)/tau)\n"
     ]
    }
   ],
   "source": [
    "# compute CIFAR-10 dynamics\n",
    "x_train_cifar10_np = x_train_cifar10.cpu().numpy()\n",
    "theoretical_dynamics_cifar10 = theoretical_learning_dynamics(x_train_cifar10_np[:num_samples, :], \n",
    "                                                        x_train_cifar10_np[:num_samples, :], \n",
    "                                                        n_epoch=epochs, lr=lr, var=0, \n",
    "                                                        reg=0, u0 = 1.5e-6)\n",
    "theoretical_dynamics_reg_cifar10 = theoretical_learning_dynamics(x_train_cifar10_np[:num_samples, :], \n",
    "                                                            x_train_cifar10_np[:num_samples, :], \n",
    "                                                            n_epoch=epochs, lr=lr, var=0, \n",
    "                                                            reg=reg_param, u0 = 1.5e-6)\n",
    "theoretical_dynamics_noise_cifar10 = theoretical_learning_dynamics(x_train_cifar10_np[:num_samples, :], \n",
    "                                                            x_train_cifar10_np[:num_samples, :], \n",
    "                                                            n_epoch=epochs, lr=lr, var=var_param, \n",
    "                                                            reg=0, u0 = 1.5e-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dynamics plot\n",
    "slices = (0, 3, 7, 15, 31)\n",
    "fig, [(ax1, ax2), (ax3, ax4)] = plt.subplots(2, 2, figsize=(12, 8), sharey='row', sharex='col')\n",
    "axes = [ax1, ax2]\n",
    "\n",
    "# plot theoretical dynamics\n",
    "ax1.plot(theoretical_dynamics_mnist[:, slices], c='blue', \n",
    "         label='Theory ($\\gamma = 0$)')\n",
    "ax1.plot(theoretical_dynamics_reg_mnist[:, slices], c='orange', \n",
    "         label='Theory ($\\gamma = $' + str(reg_param) + ')')\n",
    "ax2.plot(theoretical_dynamics_mnist[:, slices], c='blue', \n",
    "         label='Theory ($\\sigma^2 = 0$)')\n",
    "ax2.plot(theoretical_dynamics_noise_mnist[:, slices], c='darkgreen', \n",
    "         label='Theory ($\\sigma^2 = $' + str(var_param) + ')')\n",
    "\n",
    "# get actual dynamics\n",
    "actual_dynamics_mnist = mnist_models[0].strenghts.cpu().numpy()\n",
    "actual_dynamics_reg_mnist = mnist_models[1].strenghts.cpu().numpy()\n",
    "actual_dynamics_noise_mnist = mnist_models[2].strenghts.cpu().numpy()\n",
    "\n",
    "# plot simulated dynamics\n",
    "x_p = np.arange(0, epochs+1, 100)\n",
    "for s in slices:\n",
    "    ax1.scatter(x_p, actual_dynamics_mnist[:, s], c='blue', \n",
    "                marker='x', label='Actual ($\\gamma = 0$)')\n",
    "    ax1.scatter(x_p, actual_dynamics_reg_mnist[:, s], c='orange', \n",
    "                marker='x', label='Actual ($\\gamma = $' + str(reg_param) + ')')\n",
    "    ax2.scatter(x_p, actual_dynamics_mnist[:, s], c='blue', \n",
    "                marker='x', label='Actual ($\\sigma^2 = 0$)')\n",
    "    ax2.scatter(x_p, actual_dynamics_noise_mnist[:, s], c='darkgreen',\n",
    "                marker='x', label='Actual ($\\sigma^2 = $' + str(var_param) + ')')\n",
    "    \n",
    "    \n",
    "# set plot titles and axis labels\n",
    "ax1.set_ylabel('$w_2 \\cdot w_1$', fontsize=15)\n",
    "ax1.set_title('Weight decay')\n",
    "ax2.yaxis.set_label_position('right')\n",
    "ax2.set_ylabel('MNIST')\n",
    "\n",
    "# remove duplicates labels\n",
    "locations = ['lower right', 'lower right', 'upper right', 'upper right']\n",
    "for ax, loc in zip(axes, locations):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = OrderedDict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc=loc)\n",
    "\n",
    "# plot theoretical dynamics\n",
    "ax3.plot(theoretical_dynamics_cifar10[:, slices], c='blue')\n",
    "ax3.plot(theoretical_dynamics_reg_cifar10[:, slices], c='orange')\n",
    "ax4.plot(theoretical_dynamics_cifar10[:, slices], c='blue')\n",
    "ax4.plot(theoretical_dynamics_noise_cifar10[:, slices], c='darkgreen')\n",
    "\n",
    "# get actual dynamics\n",
    "actual_dynamics_cifar10 = cifar10_models[0].strenghts.cpu().numpy()\n",
    "actual_dynamics_reg_cifar10 = cifar10_models[1].strenghts.cpu().numpy()\n",
    "actual_dynamics_noise_cifar10 = cifar10_models[2].strenghts.cpu().numpy()\n",
    "\n",
    "# plot simulated dynamics\n",
    "x_p = np.arange(0, epochs+1, 100)\n",
    "for s in slices:\n",
    "    ax3.scatter(x_p, actual_dynamics_cifar10[:, s], c='blue', marker='x')\n",
    "    ax3.scatter(x_p, actual_dynamics_reg_cifar10[:, s], c='orange', marker='x')\n",
    "    ax4.scatter(x_p, actual_dynamics_cifar10[:, s], c='blue', marker='x')\n",
    "    ax4.scatter(x_p, actual_dynamics_noise_cifar10[:, s], c='darkgreen',marker='x')\n",
    "\n",
    "# set plot titles and axis labels\n",
    "ax2.set_title('Noise')\n",
    "ax3.set_ylabel('$w_2 \\cdot w_1$', fontsize=15)\n",
    "ax3.set_xlabel('t (epoch)', fontsize=10)\n",
    "ax4.set_xlabel('t (epoch)', fontsize=10)\n",
    "ax4.yaxis.set_label_position('right')\n",
    "ax4.set_ylabel('CIFAR-10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
